
import random
import time
import torch
import subprocess
import json
import os
import signal
import pickle
import csv
import whisper
import speech_recognition as sr
# import rag_viper_sample_code.py_rag_offloader as pro
import streamlit as st
import numpy as np


from streamlit_lottie import st_lottie
from streamlit import _bottom
from typing import Any
from pydantic import BaseModel
from unstructured.partition.pdf import partition_pdf
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import CharacterTextSplitter
from tqdm import tqdm


path = "./parsing_data"
# embedding_model = SentenceTransformer("WhereIsAI/UAE-Large-V1")

def example_generator():

    example_list =[]
    # st.seesion_state.model_name, st.session_state.rag_on, st.session_state.rag_db

    if st.session_state.model_name == "TAIDE":
        if st.session_state.rag_on:
            if st.session_state.rag_db == "DBpedia":
                example_list =["Tell me the history of Super Mario",
                               "Tell me the history of computex",
                               "Give me a brief introduction about Apple Inc.?"]
        else:
            example_list=["å¹«æˆ‘è¦åŠƒå°åŒ—ä¸€æ—¥éŠçš„è¡Œç¨‹",
                          "è«‹å¹«æˆ‘ç¸½çµå°ç‰ˆæ™¶ç‰‡æ³•æ¡ˆ",
                          "å°ç£ä¿è‚²é¡ç‰©ç¨®"
                            ]
    else:
        if st.session_state.rag_on:
            if st.session_state.rag_db == "DBpedia":
                example_list =["Tell me the history of Super Mario",
                               "Tell me the history of computex",
                               "Give me a brief introduction about Apple Inc.?"]
        else:
            example_list= ["Safety Precautions for mountain climbing",
                           " Python codes for finding the prime",
                           " Tell me an interesting fact about Egypt"]
            
    return example_list

def rag_db_prepare():

    return 0

def rag_process(prompt, corpus_emb, raw_text, model):

    embedding_model = model
    # global rag_offloader
    prompt_emb = embedding_model.encode(prompt, convert_to_tensor=True)

    if st.session_state.rag_device == "viper":


        # prompt_emb_arr = torch.squeeze(prompt_emb, 0).numpy().astype(np.int8)
        scale = 0.0374
        prompt_emb = torch.clamp(torch.round(prompt_emb/scale), -127, 127).to(torch.int8)
        prompt_emb_arr = prompt_emb.view(-1, 16).flip(1).flatten().numpy()
        
        output_scores = np.zeros((4001792,), dtype=np.int8)
        print("output scores shape: ", output_scores.shape)

        # dot product on viper
        st.session_state.rag_offloader.run(prompt_emb_arr, output_scores)

        dot_result = torch.from_numpy(output_scores)
        print("dot_result shape: ", dot_result.size())

    else:
        #  print("corpus_emb dtype: ", corpus_emb.dtype)
        #  print('promt emb dtype: ', prompt_emb.dtype)
         dot_result =dot_score(corpus_emb, prompt_emb)
         print(" cpu dot_result shape: ", dot_result.size())

    values, indices = torch.topk(dot_result, 4, dim=0)

    print("values: ", values)
    # print("indices: ", indices)
    rag_ele = []
    for i in range(len(values)):
        rag_ele.append(raw_text[indices[i]])
        # print("content: ", raw_text[indices[i]])
        # print("values: ", values[i])
        # print("--"*100)

    


    # del st.session_state.rag_offloader

    return rag_ele

def dot_score(a: torch.Tensor, b: torch.Tensor):
    """
    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.
    :return: Matrix with res[i][j]  = dot_prod(a[i], b[j])
    """
    if not isinstance(a, torch.Tensor):
        a = torch.tensor(a)

    if not isinstance(b, torch.Tensor):
        b = torch.tensor(b)

    if len(a.shape) == 1:
        a = a.unsqueeze(0)

    if len(b.shape) == 1:
        b = b.unsqueeze(0)

    return torch.mm(a, b.transpose(0, 1))#torch.round(torch.mm(a, b.transpose(0, 1))*35*2/2**16/2).to(torch.int8)


# from transformers import LlamaForCausalLM, LlamaTokenizer
def check_session_state():
    black_list = ['messages', 'real_prompt', 'corpus_emb', 'raw_text', 'history']
    print("="*100)
    for key in st.session_state.keys():
        if key not in black_list:
            print("key: ", key, end="  =>  ")
            print(st.session_state[key])
    print("="*100)

def consume_output(process):
    while st.session_state.running_state:
        # print("running !!")
        try:
            char = process.stdout.read(1)#.read(1)
            if not char:
                break
            # gen_temp_img.empty()
            lottie_container.empty()
            yield char
        except:
            break

def clear_text(text):
    return text.replace("'", "'\\''")

def shellquote(s, sys_prompt, rag_element=None):

    # multi_round = "<s>[INST] <<SYS>>\n{sys}\n<</SYS>>\n\n{question1} [/INST] {model_answer_1} </s><s>[INST] {question2} [/INST]"
    question = ""
    system_setting = ""

    if sys_prompt != "":
        if st.session_state.model_name == "TAIDE" :
            # '''for taide '''
            system_setting = sys_prompt + "ï¼Œåªæœƒç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œï¼Œç›¡å¯èƒ½ç°¡çŸ­åœ°å›ç­”"
        elif st.session_state.model_name == "Llama-2-7B":
            # '''for llama '''
            system_setting = sys_prompt + ", aiming to keep your answers as brief as possible ." #, please respond as briefly as possible.
    else:
        if st.session_state.model_name == "TAIDE" :
            # '''for taide '''
            system_setting = "ä½ æ˜¯ä¸€å€‹ä¾†è‡ªå°ç£çš„AIåŠ©ç†ï¼Œä½ çš„åå­—æ˜¯ TAIDEï¼Œæ¨‚æ–¼ä»¥å°ç£äººçš„ç«‹å ´å¹«åŠ©ä½¿ç”¨è€…ï¼Œåªæœƒç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œï¼Œç›¡å¯èƒ½ç°¡çŸ­åœ°å›ç­”"
        elif st.session_state.model_name == "Llama-2-7B":
            # '''for llama '''
            system_setting = "You are a helpful AI assistant, aiming to keep your answers as brief as possible ." #, please respond as briefly as possible.


    
    # print("system setting: ", system_setting)
    question = s

    if rag_element:

        context = ""

        for i in rag_element:
            print(i)
            context = context + i + "\n"

        if st.session_state.model_name == "TAIDE" :
            system_setting = system_setting + "ã€‚ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±èªªä½ ä¸çŸ¥é“ï¼Œä¸è¦è©¦åœ–ç·¨é€ ç­”æ¡ˆã€‚"
            question = "Context: " + context +"Question: " + question + "åªå›è¦†æœ‰ç”¨çš„ç­”æ¡ˆï¼Œä¸å›è¦†ä»»ä½•å…¶ä»–å…§å®¹ã€‚"
        elif st.session_state.model_name == "Llama-2-7B":
            system_setting = system_setting + " Use the following context to answer the user's question. If you don't know the answer, just say that you don't know, don't try to make up an answer."
            question = "Context: " + context +"Question: " + question + "Only return the helpful answer below and nothing else."
        
    system_setting = system_setting.replace("'", "'\\''")
    question = question.replace("'", "'\\''")
    
    
    


    # memory 
    prompt_template = "<s>[INST] <<SYS>>\n"+system_setting+"\n<</SYS>>\n\n"+question+" [/INST]"
    # else:
    if len(st.session_state.history) == 0 or st.session_state.rag_on == True:
        prompt_template = "<s>[INST] <<SYS>>\n"+system_setting+"\n<</SYS>>\n\n"+question+" [/INST]"
    else:
        
       prompt_template = "<s>[INST] <<SYS>>\n"+system_setting+"\n<</SYS>>\n\n"+clear_text(st.session_state.history[0]["prompt"])+" [/INST] " \
        + clear_text(st.session_state.history[0]['content'])+" </s><s>[INST] "+question +" [/INST]"
        # prompt_template = "<s>[INST] <<SYS>>\n"+system_setting+"\n<</SYS>>\n\n"+question+" [/INST]"
    
    return prompt_template #"'"+ prompt_template + "'"

def session_state_init():
    if "model_name" not in st.session_state:
        st.session_state.model_name=""

    if "model_select" not in st.session_state:
        st.session_state.model_select="TAIDE"

    if "model_path" not in st.session_state:
        st.session_state.model_path = "../../output_weight/taide_lx_7b_chat/taide_llama2"

    if "system_prompt" not in st.session_state:
        st.session_state.system_prompt = ""

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "real_prompt" not in st.session_state:
        st.session_state.real_prompt = ""

    if "current_pid" not in st.session_state:
        st.session_state.current_pid = None
    # '''model path init '''
    # st.session_state.model_path = "../../output_weight/taide_lx_7b_chat/taide_llama2"
    if "running_state" not in st.session_state: 
        st.session_state.running_state = False
    
    if "rag_on" not in st.session_state:
        st.session_state.rag_on = False
    
    if "rag_filename" not in st.session_state:
        st.session_state.rag_filename = None
    
    if "rag_db" not in st.session_state:
        st.session_state.rag_db="customized"

    if "rag_customized_db_done" not in st.session_state:
        st.session_state.rag_customized_db_done = False
    
    if "history_counter" not in st.session_state:
        st.session_state.history_counter = 0
    
    if "history" not in st.session_state:
        st.session_state.history = []

    if "rag_device" not in st.session_state:
        st.session_state.rag_device = "cpu"

    if "rag_viper_loaded" not in st.session_state:
        st.session_state.rag_viper_loaded = False

    if "corpus_emb" not in st.session_state:
        st.session_state.corpus_emb = []

    if "raw_text" not in st.session_state:
        st.session_state.raw_text = []

    if "rag_offloader" not in st.session_state:
        st.session_state.rag_offloader = None

    # if "rag_viper_loaded" not in st.session_state:
    #     st.session_state.rag_viper_loaded = False
    
    if "llm_engine" not in st.session_state:
        st.session_state.llm_engine = "viper"
    
    if "viper_weight_downloaded" not in st.session_state:
        st.session_state.viper_weight_downloaded = False

    if "rec_prompt" not in st.session_state:
        st.session_state.rec_prompt = ""
    
    if "valid_rec_prompt" not in st.session_state:
        st.session_state.valid_rec_prompt = False

    if "example_prompt_pressed" not in st.session_state:
        st.session_state.example_prompt_pressed = ""
    
    if "running_process" not in st.session_state:
        st.session_state.running_process = None

def stop_btn_click_cb():

    if st.session_state.running_state and st.session_state.current_pid != None:
            print("befor kill PID: ", st.session_state.current_pid)
            os.killpg(os.getpgid(st.session_state.current_pid), signal.SIGUSR1)
            os.killpg(os.getpgid(st.session_state.current_pid+1), signal.SIGUSR1)
            # st.session_state.running_process.terminate()
            st.session_state.current_pid = None
            time.sleep(0.1)
            st.session_state.running_process = None
            st.session_state.running_state = False
            st.session_state.valid_rec_prompt = False
            st.session_state.example_prompt_pressed = ""

    
    stop_btn_container.empty()
    # os.killpg(os.getpgid(st.session_state.current_pid), signal.SIGUSR1)
    # os.killpg(os.getpgid(st.session_state.current_pid+1), signal.SIGUSR1)
    # st.session_state.current_pid = None
    # time.sleep(0.1)
    # st.session_state.running_state = False
    return


def clear_btn_click_cb():  
    st.session_state.messages= [] 
    st.session_state.history = []
    st.session_state.history_counter = 0 
    return

def record_btn_click_cb(recognizer, whisper_model):

    if st.session_state.running_state and st.session_state.current_pid != None:
            print("befor kill PID: ", st.session_state.current_pid)
            os.killpg(os.getpgid(st.session_state.current_pid), signal.SIGUSR1)
            os.killpg(os.getpgid(st.session_state.current_pid+1), signal.SIGUSR1)
            # st.session_state.running_process.terminate()
            st.session_state.current_pid = None
            time.sleep(0.1)
            st.session_state.running_process = None
            st.session_state.running_state = False
            st.session_state.valid_rec_prompt = False
            st.session_state.example_prompt_pressed = ""

    AUDIO_FILE = "test.wav"
    # activate mic to recording
    with sr.Microphone() as source:
        #noise calibration
        # r.adjust_for_ambient_noise(source)
        print("Pelease Say something~")
        audio = recognizer.listen(source, phrase_time_limit=15)
        print("Recording completed!")
        

    #print("type of audio object: ", type(audio))
    #save to file
    with open("./"+AUDIO_FILE, "wb") as file:
        file.write(audio.get_wav_data())
        file.close()


    print("===========================================")
    st.session_state.rec_prompt  = whisper_model.transcribe("test.wav")['text']
    # st.session_state.rec_prompt = "it's the recorded text: haha"

    # set this as the last step
    st.session_state.valid_rec_prompt = True
    return

@st.cache_resource
def loading_emb_model():
    return SentenceTransformer("WhereIsAI/UAE-Large-V1")

@st.cache_resource
def loading_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def loading_recognizer():
    return sr.Recognizer()



    

if __name__ == "__main__":

    st.markdown("<h1 style='text-align: center; color: white;'>RAG-LLM All on Viper</h1>", unsafe_allow_html=True)
    # st.title("RAG-LLM All on Viper")
    # header = st.container()
    # header.title("RAG-LLM All on Viper")
    # header.write("""<div class='fixed-header'/>""", unsafe_allow_html=True)

    # ### Custom CSS for the sticky header
    # st.markdown(
    #     """
    # <style>
    #     div[data-testid="stVerticalBlock"] div:has(div.fixed-header) {
    #         position: sticky;
    #         top: 2.875rem;
    #         background-color: black;
    #         text-align: center;
    #         z-index: 999;
    #     }
    #     .fixed-header {
    #         border-bottom: 1px solid black;
    #         text-align: center;
    #     }
    # </style>
    #     """,
    #     unsafe_allow_html=True
    # )
        
    session_state_init()
    check_session_state()

    # params
    db_path = "../../db_folder/"
    save_path = "../../uploaded_files/"
    viper_file_path = "./rag_viper_sample_code/"
    target_viper = "neuchips_ai_epr-1"
    config = "config.csv"
    preload_ddr_data = "ddr_data.csv"
    test_data = "test_data.csv"
    # corpus_emb = None
    # raw_text = None

    embedding_model = loading_emb_model()#SentenceTransformer("WhereIsAI/UAE-Large-V1")
    whisper_model = loading_whisper_model()
    recognizer =loading_recognizer()
    recognizer.energy_threshold = 3000


    st.markdown(
        """
        <style>
            [data-testid=stSidebar] [data-testid=stImage]{
                text-align: center;
                display: block;
                margin-left: auto;
                margin-right: auto;
                width: 100%;
            }
        </style>
        """, unsafe_allow_html=True
    )


    with st.sidebar:
        
        # pick your model
        support_model_list = ['TAIDE', "Llama-2-7B"]

        st.image("./company_logo/Logo_NEUCHIPS_v_c+w.png", width = 300)

        # st.title("RAG-LLM on Viper")
        # st.write("Select your LLM")
        value = st.selectbox("Select your model", support_model_list)
        # print("last model: ", last_model)

        if value == "TAIDE" and st.session_state.model_name != "TAIDE":
            
            # st.balloons()
            
            # with st.spinner("Downloading weight to N3000 ..."):
            #     p = subprocess.check_output(" ./neu_weight_util --weight ../../output_weight/taide_lx_7b_chat/ --n3kid 0", shell=True, stderr=subprocess.STDOUT)

            #     # st.success(" TAIDE is ready !")
            st.session_state.messages = []
            st.session_state.history = []
            st.session_state.real_prompt = ""
            st.session_state.model_name = "TAIDE"
            
            # st.session_state.model_path = "../../output_weight/taide_lx_7b_chat/taide_llama2"
            
        elif value  == "Llama-2-7B" and st.session_state.model_name != "Llama-2-7B":
            
            # with st.spinner("Downloading weight to N3000 ..."):
                
            #     p = subprocess.check_output("./neu_weight_util --weight ../../output_weight/7B-chat/ --n3kid 0", shell=True, stderr=subprocess.STDOUT)
            #     # st.success("Llama-2-7B is ready !")
            st.session_state.messages = []
            st.session_state.history = []
            st.session_state.real_prompt = ""
            st.session_state.model_name = "Llama-2-7B"
            # st.session_state.model_path = "../../output_weight/7B-chat/llama2_7b_cht"

        st.session_state.model_select = value

        
        with st.container():
            # st.write("LLM Inference Engine")
            llm_engine = st.radio(
                "Select LLM Engine ğŸ‘‡",
                ["CPU", "Neuchips Viper"],
                key="llm_hardware",
                label_visibility="visible",
                disabled=False,
                horizontal=True,
            )

            if llm_engine == "Neuchips Viper":
                st.session_state.llm_engine = "viper"

                if st.session_state.model_name == "TAIDE":
                    st.session_state.model_path = "../../output_weight/taide_lx_7b_chat/taide_llama2"

                    if st.session_state.model_select != "TAIDE" or not st.session_state.viper_weight_downloaded :
                        with st.spinner("Downloading weight to N3000 ..."):
                            p = subprocess.check_output(" ./neu_weight_util --weight ../../output_weight/taide_lx_7b_chat/ --n3kid 0", shell=True, stderr=subprocess.STDOUT)
                            st.session_state.viper_weight_downloaded = True
                else:
                    st.session_state.model_path = "../../output_weight/7B-chat/llama2_7b_cht"

                    if st.session_state.model_select != "Llama-2-7B" or not st.session_state.viper_weight_downloaded :
                        with st.spinner("Downloading weight to N3000 ..."):
                            p = subprocess.check_output("./neu_weight_util --weight ../../output_weight/7B-chat/ --n3kid 0", shell=True, stderr=subprocess.STDOUT)
                            st.session_state.viper_weight_downloaded = True
            else:
                st.session_state.llm_engine="cpu"
                if st.session_state.model_name == "TAIDE":
                    st.session_state.model_path = "../../cpu_model/taide_7b.gguf"
                else:
                    st.session_state.model_path = "../../cpu_model/llama2_7b_chat.gguf"
            

        # with st.container():#st.form("my_form"):
        @st.dialog("Character Design")
        def char_design():
            system_prompt = st.text_area("Naming your Assistant", "")
            if st.button("Submit"):
                if system_prompt:
                    st.session_state.system_prompt = system_prompt
                st.rerun()

        # st.write("Design Your Own LLM")
        with st.container():
            st.write("Design Your Own LLM")
            if st.button("Character Design", use_container_width=True):
                char_design()



        on = st.toggle("RAG Function")

        # rag state change => clear history
        if on != st.session_state.rag_on:
            print("clear")
            st.session_state.history = []
            st.session_state.history_counter = 0

        st.session_state.rag_on = on

        with st.container(border=True):
            
        
            check_session_state()
            choice = st.radio(
                "Select Database ğŸ‘‡",
                ["DBpedia", "Create DB"],
                key="visibility",
                label_visibility="visible",
                disabled=not st.session_state.rag_on,
                horizontal=True,
            )
            print("choice: ", choice)
            
            if choice == "Create DB":
                st.session_state.rag_db="customized"
                uploaded_file = st.file_uploader("Choose a PDF file", accept_multiple_files=False, label_visibility="hidden")
                
                if uploaded_file is not None:
                    print("puloade_file")
                    byte_data = uploaded_file.read()
                    saved_name =uploaded_file.name
                    with open(save_path+saved_name, 'wb') as f: 
                        f.write(byte_data)
                    print('save_name: ', saved_name)
                    st.session_state.rag_filename = saved_name
                    st.session_state.rag_device = "cpu"

            elif choice == "DBpedia":
                st.session_state.rag_db="DBpedia"
                st.image("./company_logo/DBpediaLogo.png", width=150) 
            

                # device_choice = st.radio(
                #         "Select Device ",
                #         ["CPU", "Neuchips Viper"],
                #         key="device_radio",
                #         label_visibility="visible",
                #         disabled= not (True if st.session_state.rag_db == 'DBpedia' else False) or not st.session_state.rag_on,
                #         horizontal=True,
                #     )
                device_choice = "CPU"
                
                if device_choice == "Neuchips Viper":
                    # st.session_state.rag_device = "viper"

                    # set to None to disable viper device
                    devices = None #pro.get_available_device()
                    
                    # make sure viper for rag is ready
                    print("target_viper: ", target_viper)
                    if target_viper not in devices and not st.session_state.rag_viper_loaded:
                        print("Error. The device ", target_viper, " is not available.")
                        st.error("Error. The device Neuchips Viper is not available.", icon="ğŸš¨")
                        st.session_state.rag_device = "cpu"

                    else:
                        # st.success('Neuchips Viper is Ready', icon="âœ…")
                        st.session_state.rag_device ="viper"

                else:
                    st.session_state.rag_device = "cpu"
                

            submitted = st.button("Submit", use_container_width=True, disabled= not st.session_state.rag_on)
            if submitted:
                # st.session_state.rag_on = True
            
                
                with st.spinner("Preparing RAG Database ~"):

                    if st.session_state.rag_db == "DBpedia":
                        if st.session_state.rag_device == 'viper':

                            # st.session_state.rag_offloader = None

                            

                            
                            # org code
                            rag_offloader = pro.PyRagOffloader()
                            rag_offloader.bind_device(target_viper)
                            rag_offloader.set_config(db_path+"dbpedia/"+config)
                            rag_offloader.init_ddr(db_path+"dbpedia/"+preload_ddr_data)
                            

                            st.session_state.rag_offloader = rag_offloader
                            del rag_offloader

                            if not st.session_state.rag_offloader.is_valid():
                                st.error("Error. Can not create rag offloader.")
                                
                            st.session_state.rag_viper_loaded = True

                            # preload DB in to Viper
                        else:
                            # loading corpus
                            corpus_emb = torch.load(db_path+"dbpedia/dbpedia_corpus_emb.pt", map_location=torch.device('cpu') ).to(torch.float32)
                            
                            #loading raw text
                        
                        raw_text = torch.load(db_path+"dbpedia/dbpedia_raw_text.pt", map_location=torch.device('cpu') )#.to(torch.float16)

                    elif st.session_state.rag_db == "customized":
                        basename = os.path.splitext(st.session_state.rag_filename)[0]
                        save_db_name = basename +".pt"

                        if os.path.isfile(db_path+basename+'_corpus_emb.pt'):
                            corpus_emb = torch.load(db_path+basename+"_corpus_emb.pt")
                            raw_text = torch.load(db_path+basename+"_raw_text.pt")
                        else:
                            pdf_start = time.time()
                            raw_pdf_elements = partition_pdf(
                                filename= save_path + st.session_state.rag_filename,
                                # Unstructured first finds embedded image blocks
                                extract_images_in_pdf=False,
                                # in
                                # Titles are any sub-section of the document
                                infer_table_structure=True,
                                # Post processing to aggregate text once we have the title
                                chunking_strategy="by_title",
                                # Chunking params to aggregate text blocks
                                # Attempt to create a new chunk 3800 chars
                                # Attempt to keep chunks > 2000 chars
                                max_characters=400,
                                new_after_n_chars=380,
                                combine_text_under_n_chars=200,
                                image_output_dir_path="./parsing_data",
                            )
                            print("pdf parsing time: ", time.time()-pdf_start)
                            

                            # Create a dictionary to store counts of each type
                            category_counts = {}

                            

                            class Element(BaseModel):
                                type: str
                                text: Any


                            # Categorize by type
                            categorized_elements = []
                            for element in raw_pdf_elements:
                                if "unstructured.documents.elements.Table" in str(type(element)):
                                    categorized_elements.append(Element(type="table", text=str(element)))
                                elif "unstructured.documents.elements.CompositeElement" in str(type(element)):
                                    categorized_elements.append(Element(type="text", text=str(element)))

                            # Tables
                            table_elements = [e for e in categorized_elements if e.type == "table"]
                            
                            text_elements = [e for e in categorized_elements if e.type == "text"]
                        

                            raw_text = []

                            corpus_start = time.time()
                            for i in tqdm(range(len(text_elements))):
                                raw_text.append(text_elements[i].text)

                            for t in tqdm(range(len(table_elements))):
                                raw_text.append(table_elements[t].text)

                            # embedding_model = SentenceTransformer("WhereIsAI/UAE-Large-V1")
                            corpus_emb = embedding_model.encode(raw_text, convert_to_tensor=True)
                            torch.save(corpus_emb, db_path+basename+"_corpus_emb.pt")
                            torch.save(raw_text, db_path+basename+"_raw_text.pt")
                            # st.session_state.rag_customized_db_done = True
                            print(corpus_emb.size())
                            # print(Find_scale(corpus_emb))
                            print("get corpus db time: ", time.time() - corpus_start)

                    if st.session_state.rag_device != "viper":
                        st.session_state.corpus_emb = corpus_emb
                    
                    st.session_state.raw_text = raw_text
                    st.success('RAG Database is Ready', icon="âœ…")
             

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar= "â‡ï¸"if message["role"] == "assistant" else "ğŸ§™â€â™€ï¸"):
            st.markdown(message["content"])


    button_cols = _bottom.columns(3)

    if (st.session_state.rag_db != "customized" and st.session_state.rag_on) or not st.session_state.rag_on:
        example_prompts = example_generator()
        

        if button_cols[0].button(example_prompts[0], use_container_width=True):
            st.session_state.example_prompt_pressed = example_prompts[0]


        if button_cols[1].button(example_prompts[1], use_container_width=True):
            if example_prompts[1] == "è«‹å¹«æˆ‘ç¸½çµå°ç‰ˆæ™¶ç‰‡æ³•æ¡ˆ":
                st.session_state.example_prompt_pressed  = "è«‹å°‡é€™ç¯‡æ–‡ç« ç²¾ç°¡æ¢ç†åŒ–:ã€Œç”¢æ¥­å‰µæ–°æ¢ä¾‹ç¬¬10æ¢ä¹‹2åŠç¬¬72æ¢æ¢æ–‡ä¿®æ­£æ¡ˆã€ä¿—ç¨±ã€Œå°ç‰ˆæ™¶ç‰‡æ³•ã€,é‡å°åŠå°é«”ã€é›»å‹•è»Šã€5Gç­‰æŠ€è¡“å‰µæ–°ä¸”å±…åœ‹éš›ä¾›æ‡‰éˆé—œéµåœ°ä½å…¬å¸,æä¾›æœ€é«˜25%ç‡Ÿæ‰€ç¨…æŠ•æŠµå„ªæƒ ,ä¼æ¥­é©ç”¨è¦ä»¶åŒ…å«ç•¶å¹´åº¦ç ”ç™¼è²»ç”¨ã€ç ”ç™¼å¯†åº¦é”ä¸€å®šè¦æ¨¡,ä¸”æœ‰æ•ˆç¨…ç‡é”ä¸€å®šæ¯”ç‡ã€‚\
ç‚ºå› æ‡‰ç¶“æ¿Ÿåˆä½œæš¨ç™¼å±•çµ„ç¹”(OECD)åœ‹å®¶æœ€ä½ç¨…è² åˆ¶èª¿æ•´,å…¶ä¸­æœ‰æ•ˆç¨…ç‡é–€æª»,æ°‘åœ‹112å¹´è¨‚ç‚º12%,113å¹´æ–™å°‡æé«˜è‡³15%,ä½†ä»å¾—å¯©é…Œåœ‹éš›é–“æœ€ä½ç¨…è² åˆ¶å¯¦æ–½æƒ…å½¢ã€‚\
ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,å·²å’Œè²¡æ”¿éƒ¨å”å•†é€²å…¥æœ€å¾Œéšæ®µ,é™¤ä¼æ¥­ç ”ç™¼å¯†åº¦è¨‚åœ¨6%,ç›®å‰å·²ç¢ºèª,ä¼æ¥­è³¼ç½®å…ˆé€²è£½ç¨‹çš„è¨­å‚™æŠ•è³‡é‡‘é¡é”100å„„å…ƒä»¥ä¸Šå¯æŠµæ¸›ã€‚\
è²¡æ”¿éƒ¨å®˜å“¡è¡¨ç¤º,ç ”å•†éç¨‹ä¸­,é‡å°å°ç£ç”¢æ¥­èˆ‡å…¶åœ¨åœ‹éš›é–“é¡ä¼¼çš„å…¬å¸é€²è¡Œæ·±å…¥ç ”ç©¶,åœ¨è¨­å‚™éƒ¨åˆ†,ç•¢ç«Ÿé©ç”¨ç”¢å‰µ10ä¹‹2çš„æ¥­è€…æ˜¯ä»£è¡¨å°ç£éšŠæ‰“ã€Œåœ‹éš›ç›ƒã€,æŠ•å…¥é‡‘é¡ä¸é”100å„„å…ƒ,å¯èƒ½ä¹Ÿæ‰“ä¸äº†ã€‚\
è‡³æ–¼å‚™å—é—œæ³¨çš„ç ”ç™¼è²»ç”¨é–€æª»,ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,æ­·ç¶“èˆ‡è²¡æ”¿éƒ¨ä¾†å›å¯†åˆ‡è¨è«–,ç ”ç™¼è²»ç”¨é–€æª»æœ‰æœ›è½åœ¨60å„„è‡³70å„„å…ƒä¹‹é–“ã€‚\
è²¡æ”¿éƒ¨å®˜å“¡æŒ‡å‡º,ç ”ç™¼æ”¸é—œå°ç£æœªä¾†ç¶“æ¿Ÿæˆé•·å‹•èƒ½,é–€æª»ä¸èƒ½ã€Œé«˜ä¸å¯æ”€ã€,èµ·åˆé›–è¨­å®šåœ¨100å„„å…ƒ,ä¹‹æ‰€ä»¥æœƒèª¿é™,æ­£æ˜¯ç›¼è®“ä¼æ¥­è¦ºå¾—æœ‰è¾¦æ³•é”å¾—åˆ°é–€æª»ã€é€²è€Œé©ç”¨ç§Ÿç¨…å„ªæƒ ,æ‰æœ‰å‹•åŠ›ç¹¼çºŒæŠ•å…¥ç ”ç™¼,ç¶­æŒåœ‹éš›ä¾›æ‡‰éˆé—œéµåœ°ä½ã€‚\
ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,å› å» å•†ç ”ç™¼è²»ç”¨å¹³å‡ç‚º30ã€40å„„å…ƒ,å…¶ä¸­,ICè¨­è¨ˆæ¥­è€…ä»‹æ–¼30å„„è‡³60å„„å…ƒç¯„åœ,è‹¥å°‡é–€æª»è¨‚åœ¨100å„„å…ƒ,ç¬¦åˆæ¢ä»¶çš„æ¥­è€…è¼ƒå°‘ã€åˆºæ¿€èª˜å› ä¸è¶³;æ­¤å¤–,è‹¥ç¬¦åˆç”³è«‹é–€æª»çš„æ¥­è€…å¢åŠ ,å°‡å¯æé«˜ä¼æ¥­åœ¨å°æŠ•è³‡é‡‘é¡,è²¡æ”¿éƒ¨ç¨…æ”¶ä¹Ÿèƒ½å› æ­¤ç²å¾—æŒ¹æ³¨ã€‚\
ICè¨­è¨ˆæ¥­è€…è¿‘æ—¥é »é »é‡å°ç”¢å‰µ10ä¹‹2ç™¼è²,å¸Œæœ›é™ä½é©ç”¨é–€æª»,åŠ ä¸Šå„åœ‹åŠ›æ‹šä¾›æ‡‰éˆè‡ªä¸»åŒ–ã€åŠ ç¢¼è£œåŠ©åŠå°é«”ç”¢æ¥­,ç¶“æ¿Ÿéƒ¨å®˜å“¡è¡¨ç¤º,ç¶“æ¿Ÿéƒ¨å’Œè²¡æ”¿éƒ¨å°±ç”¢å‰µ10ä¹‹2é”æˆå…±è­˜,çˆ­å–è®“æ›´å¤šæ¥­è€…å—æƒ ,ç›¼å¢å¼·ä¼æ¥­æŠ•è³‡åŠ›é“åŠéå›ºå°ç£æŠ€è¡“åœ°ä½ã€‚\
è²¡æ”¿éƒ¨å®˜å“¡è¡¨ç¤º,ç§Ÿç¨…çå‹µçš„åˆ¶å®šå¿…é ˆã€Œæœ‰ç‚ºæœ‰å®ˆã€,ä¸¦ä»¥é”åˆ°çå‹µè¨­ç½®ç›®çš„ç‚ºæœ€é«˜åŸå‰‡,ç¾éšæ®µåœ¨æ‰“ã€Œåœ‹å…§ç›ƒã€çš„ä¼æ¥­ä»å¯é©ç”¨ç”¢å‰µç¬¬10æ¢ã€10ä¹‹1çš„ç§Ÿç¨…å„ªæƒ ,å…±åŒå£¯å¤§å°ç£ç¶“æ¿Ÿç™¼å±•ã€‚\
ç¶“æ¿Ÿéƒ¨å’Œè²¡æ”¿éƒ¨æ­£å°±ç ”ç™¼è²»ç”¨é–€æª»åšæœ€å¾Œç¢ºèª,å¾…ä»Šæ˜å…©å¤©é å‘Šå­æ³•ä¹‹å¾Œ,ç´„æœ‰30å¤©æ™‚é–“,å¯èˆ‡æ¥­ç•Œé€²ä¸€æ­¥è¨è«–åŠèª¿æ•´,ç›¼ç”¢å‰µ10ä¹‹2èƒ½åœ¨6æœˆä¸Šè·¯ã€‚"
            else:
                st.session_state.example_prompt_pressed  = example_prompts[1]

        if button_cols[2].button(example_prompts[2], use_container_width=True):
            st.session_state.example_prompt_pressed = example_prompts[2]

    cols = _bottom.columns([1.9, 17.2, 2.5, 2.6], gap="small")

    with cols[1]:
        # st.session_state._pprompt = st.chat_input("What is up?")
        # print("prompt: ", st.session_state._pprompt)
        prompt = st.chat_input("What is up?", key="chat_in")
        

    
    with cols[0]:
       record_btn = st.button(":studio_microphone:", use_container_width=True)


    if record_btn:

        record_msg_container= st.empty()
        with record_msg_container:
            st.markdown("recording~")
            record_btn_click_cb(recognizer, whisper_model)
            record_msg_container.empty()

    # for whisper
    if st.session_state.valid_rec_prompt:
        prompt = st.session_state.rec_prompt

    if st.session_state.example_prompt_pressed !="":
        prompt = st.session_state.example_prompt_pressed
    
    print("prompt: ", prompt)
    

    if prompt: #:= st.chat_input("What is up?"):
        # check_session_state()
        

        if st.session_state.running_state and st.session_state.current_pid != None:
            print("befor kill PID: ", st.session_state.current_pid)
            os.killpg(os.getpgid(st.session_state.current_pid), signal.SIGUSR1)
            os.killpg(os.getpgid(st.session_state.current_pid+1), signal.SIGUSR1)
            # st.session_state.running_process.terminate()
            st.session_state.current_pid = None
            time.sleep(0.1)
            st.session_state.running_process = None
            st.session_state.running_state = False
            st.session_state.valid_rec_prompt = False
            st.session_state.example_prompt_pressed = ""
            


        # RAG mlcommon
        if st.session_state.rag_on:
            print("rag on !")
            rag_element = rag_process(prompt, st.session_state.corpus_emb, st.session_state.raw_text, embedding_model)
            print("rag process done !")
            # st.session_state.rag_offloader = None
            # for i in rag_element:
            #     print(i)
            #     print("--"*100)
            word_esc = shellquote(prompt, st.session_state.system_prompt, rag_element)
        else:
            word_esc = shellquote(prompt, st.session_state.system_prompt)

        # print("word_esc: ", word_esc)
        


    
        while st.session_state.running_state == False:

            if st.session_state.llm_engine == 'viper':
                process = subprocess.Popen("./main --n3k_id %d -m %s -c 4096 -p %s -n %d -b 32 --temp 0 --no-display-prompt" % (0, st.session_state.model_path, "'"+word_esc+"'", 512), stdout=subprocess.PIPE, text=True, shell=True, close_fds=True ,preexec_fn = os.setsid)
            else:
                process = subprocess.Popen("./cpu_main -m %s -c 4096 -p %s -n %d -b 32 --temp 0 --prompt-print-skip" % (st.session_state.model_path, "'"+word_esc+"'", 512), stdout=subprocess.PIPE, text=True, shell=True, close_fds=True ,preexec_fn = os.setsid)
                
            # st.session_state.running_process = process
            st.session_state.running_state = True
            st.session_state.current_pid = process.pid

            
            
            with cols[2]:
                stop_btn_container = st.empty()
                with stop_btn_container:
                    stop_btn = st.button("Stop", on_click=stop_btn_click_cb, key=f"abc", use_container_width=True)

            
            print("current process id: ", process.pid)

    
            with st.chat_message("user", avatar="ğŸ§™â€â™€ï¸"):
                st.markdown(prompt)

            # Display assistant response in chat message container
            with st.chat_message("assistant", avatar="â‡ï¸"):
                # print("Before write stream runnning state check: ", st.session_state.running_state)
                lottie_container = st.empty()

                    # gen_temp_img = st.image("./company_logo/Logo_NEUCHIPS_h_c+w.png", width=300)
                    # with st.echo():
                    # gen_temp_img = st_lottie("./UI_elements/loading.json", height=200, width=300)
                with lottie_container:
                    with open("./UI_elements/processing_animation.json", "r") as f:
                        data = json.load(f)

                    st.markdown("""
                            <style>
                                iframe {
                                    justify-content: center;
                                    display: block;
                                    border-style:none;
                                }
                            </style>
                            """, unsafe_allow_html=True
                        )
                    st_lottie(data, height=80, key='gen')
                    # st_lottie("./processing.json")
                
                    response = st.write_stream(consume_output(process))
                    stop_btn_container.empty()
                
                print("After write stream runnning state check: ", st.session_state.running_state)
                
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.session_state.messages.append({"role": "assistant", "content": response})
            
                    

            
            print("before history counter")
            check_session_state()
            st.session_state.history_counter += 1
            
            if st.session_state.history_counter < 2:
                st.session_state.history.append({"prompt": prompt, "content": response})
            else:
                st.session_state.history.append({"prompt": prompt, "content": response})
                print("st.session_state.history len: ", len(st.session_state.history))
                del st.session_state.history[0]
                print("after del first st.session_state.history len: ", len(st.session_state.history))
                st.session_state.history_counter = 1

            print("after history counter ")
            check_session_state()

            # st.session_state.real_prompt = word_esc + clear_text(response) + "</s>"
            
            # st.session_state.running_state = False
        
        st.session_state.current_pid = None
        st.session_state.running_state = False
        st.session_state.valid_rec_prompt = False
        st.session_state.example_prompt_pressed=""

        if len(st.session_state.messages) != 0:
            with cols[3]:
                st.button("clear", on_click=clear_btn_click_cb, use_container_width=True)

        
        # print("call empty()")
        # placeholder.empty()
